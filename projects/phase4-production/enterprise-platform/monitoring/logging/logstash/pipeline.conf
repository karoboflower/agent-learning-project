# Logstash管道配置

input {
  # 从Filebeat接收日志
  beats {
    port => 5044
    codec => json
  }

  # 从TCP接��日志
  tcp {
    port => 5000
    codec => json_lines
  }

  # 从HTTP接收日志
  http {
    port => 8080
    codec => json
  }

  # 从Kafka接收日志（可选）
  # kafka {
  #   bootstrap_servers => "kafka:9092"
  #   topics => ["app-logs"]
  #   codec => json
  #   group_id => "logstash-consumer"
  # }
}

filter {
  # 解析JSON日志
  if [message] =~ /^{.*}$/ {
    json {
      source => "message"
      target => "parsed"
    }

    # 提升字段到顶层
    if [parsed] {
      mutate {
        rename => {
          "[parsed][timestamp]" => "timestamp"
          "[parsed][level]" => "level"
          "[parsed][logger]" => "logger"
          "[parsed][message]" => "log_message"
          "[parsed][service]" => "service"
          "[parsed][version]" => "version"
          "[parsed][env]" => "env"
          "[parsed][tenant_id]" => "tenant_id"
          "[parsed][user_id]" => "user_id"
          "[parsed][request_id]" => "request_id"
          "[parsed][trace_id]" => "trace_id"
          "[parsed][span_id]" => "span_id"
          "[parsed][method]" => "method"
          "[parsed][path]" => "path"
          "[parsed][status]" => "status"
          "[parsed][duration]" => "duration"
          "[parsed][error]" => "error"
          "[parsed][stacktrace]" => "stacktrace"
          "[parsed][caller]" => "caller"
        }
        remove_field => ["parsed"]
      }
    }
  }

  # 解析时间戳
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
      target => "@timestamp"
    }
  }

  # 标准化日志级别
  if [level] {
    mutate {
      lowercase => ["level"]
    }
  }

  # 添加地理位置信息（如果有IP）
  if [remote_addr] {
    geoip {
      source => "remote_addr"
      target => "geo"
    }
  }

  # 添加字段类型转换
  if [status] {
    mutate {
      convert => {
        "status" => "integer"
      }
    }
  }

  if [duration] {
    mutate {
      convert => {
        "duration" => "float"
      }
    }
  }

  # 添加标签
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "app-logs"
    }
  }

  # 根据日志级别添加标签
  if [level] == "error" or [level] == "fatal" {
    mutate {
      add_tag => ["error"]
    }
  }

  if [level] == "warn" {
    mutate {
      add_tag => ["warning"]
    }
  }

  # 检测敏感信息并脱敏
  if [log_message] =~ /(password|token|secret|key)/ {
    mutate {
      add_tag => ["sensitive"]
    }

    # 脱敏处理
    mutate {
      gsub => [
        "log_message", "password[\"':\s]*[\"']?[\w\-\.]+[\"']?", "password=***REDACTED***",
        "log_message", "token[\"':\s]*[\"']?[\w\-\.]+[\"']?", "token=***REDACTED***",
        "log_message", "secret[\"':\s]*[\"']?[\w\-\.]+[\"']?", "secret=***REDACTED***"
      ]
    }
  }

  # 提取错误信息的关键字
  if [error] {
    mutate {
      add_field => {
        "error_type" => "%{error}"
      }
    }

    # 提取错误类型
    grok {
      match => {
        "error" => "^(?<error_class>[\w\.]+):"
      }
      tag_on_failure => []
    }
  }

  # 添加处理时间戳
  ruby {
    code => "event.set('processing_time', Time.now.to_f)"
  }
}

output {
  # 输出到Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
    document_id => "%{request_id}"

    # 认证（如果启用）
    # user => "elastic"
    # password => "${ELASTICSEARCH_PASSWORD}"

    # 失败重试
    retry_on_conflict => 3

    # 批量设置
    flush_size => 500
    idle_flush_time => 5
  }

  # 错误日志输出到单独的索引
  if "error" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "error-logs-%{+YYYY.MM.dd}"
      document_id => "%{request_id}"
    }
  }

  # 输出到stdout（调试用）
  # stdout {
  #   codec => rubydebug
  # }

  # 输出到Kafka（可选）
  # kafka {
  #   bootstrap_servers => "kafka:9092"
  #   topic_id => "processed-logs"
  #   codec => json
  # }
}
