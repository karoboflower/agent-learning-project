# Fluentd配置文件（替代Filebeat+Logstash的方案）

<source>
  @type tail
  @id input_tail
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%L%z
  </parse>
  path /var/log/enterprise-platform/*.log
  pos_file /var/log/fluentd/app-logs.pos
  tag app.logs
  read_from_head true
  refresh_interval 10s
</source>

# Docker容器日志
<source>
  @type forward
  @id input_forward
  port 24224
  bind 0.0.0.0
</source>

# HTTP输入
<source>
  @type http
  @id input_http
  port 9880
  bind 0.0.0.0
  body_size_limit 32m
  keepalive_timeout 10s
</source>

# 系统日志
<source>
  @type syslog
  port 5140
  bind 0.0.0.0
  tag system
</source>

# 过滤器 - 添加字段
<filter app.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
    cluster "enterprise-platform"
  </record>
</filter>

# 过滤器 - 解析JSON
<filter app.**>
  @type parser
  key_name message
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

# 过滤器 - 脱敏
<filter app.**>
  @type record_modifier
  <replace>
    key message
    expression /password[=:]\s*\S+/
    replace password=***REDACTED***
  </replace>
  <replace>
    key message
    expression /token[=:]\s*\S+/
    replace token=***REDACTED***
  </replace>
  <replace>
    key message
    expression /secret[=:]\s*\S+/
    replace secret=***REDACTED***
  </replace>
</filter>

# 过滤器 - Kubernetes元数据
<filter kubernetes.**>
  @type kubernetes_metadata
  @id filter_kube_metadata
  kubernetes_url "#{ENV['KUBERNETES_SERVICE_HOST']}"
  verify_ssl true
  ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  skip_labels false
  skip_container_metadata false
  skip_master_url false
  skip_namespace_metadata false
</filter>

# 过滤器 - 日志级别标准化
<filter app.**>
  @type record_transformer
  enable_ruby true
  <record>
    level ${record["level"] ? record["level"].downcase : "info"}
  </record>
</filter>

# 过滤器 - 添加地理位置
<filter app.**>
  @type geoip
  geoip_lookup_keys remote_addr
  <record>
    geoip_city ${city.names.en["remote_addr"]}
    geoip_country ${country.names.en["remote_addr"]}
    geoip_location ${location.latitude["remote_addr"]},${location.longitude["remote_addr"]}
  </record>
  skip_adding_null_record true
</filter>

# 输出 - Elasticsearch（主输出）
<match app.**>
  @type elasticsearch
  @id output_elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix app-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  type_name _doc
  tag_key @log_name

  # 批量设置
  flush_interval 5s
  flush_thread_count 2
  chunk_limit_size 5M
  queue_limit_length 32
  retry_max_interval 30
  retry_forever false
  retry_max_times 3

  # 缓冲
  <buffer>
    @type file
    path /var/log/fluentd/buffer/elasticsearch
    flush_mode interval
    flush_interval 5s
    flush_at_shutdown true
    retry_type exponential_backoff
  </buffer>

  # 慢查询日志
  slow_flush_log_threshold 40.0

  # 认证（如果需要）
  # user elastic
  # password ${ELASTICSEARCH_PASSWORD}

  # SSL（如果需要）
  # scheme https
  # ssl_verify true
  # ca_file /path/to/ca.crt
</match>

# 输出 - 错误日志到单独的索引
<match app.** tag:error>
  @type elasticsearch
  @id output_elasticsearch_errors
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix error-logs
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  type_name _doc

  <buffer>
    @type file
    path /var/log/fluentd/buffer/elasticsearch-errors
    flush_interval 5s
  </buffer>
</match>

# 输出 - Kafka（可选）
# <match app.**>
#   @type kafka2
#   @id output_kafka
#   brokers kafka:9092
#   default_topic app-logs
#   <format>
#     @type json
#   </format>
#   compression_codec gzip
#   max_send_retries 3
#   required_acks -1
# </match>

# 输出 - S3归档（可选）
# <match app.**>
#   @type s3
#   @id output_s3
#   s3_bucket enterprise-platform-logs
#   s3_region us-west-2
#   path logs/
#   time_slice_format %Y%m%d%H
#   <buffer time>
#     @type file
#     path /var/log/fluentd/buffer/s3
#     timekey 3600
#     timekey_wait 10m
#   </buffer>
# </match>

# 输出 - stdout（调试用）
# <match app.**>
#   @type stdout
#   @id output_stdout
# </match>

# 监控
<source>
  @type monitor_agent
  @id input_monitor_agent
  bind 0.0.0.0
  port 24220
</source>

# Prometheus指标
<source>
  @type prometheus
  @id input_prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

<source>
  @type prometheus_output_monitor
  @id output_prometheus_monitor
  interval 10
</source>
