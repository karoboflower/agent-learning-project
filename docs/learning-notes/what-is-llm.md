# LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰è¯¦è§£

## ğŸ“š ç›®å½•

1. [ä»€ä¹ˆæ˜¯LLM](#ä»€ä¹ˆæ˜¯llm)
2. [LLMçš„å·¥ä½œåŸç†](#llmçš„å·¥ä½œåŸç†)
3. [ä¸»æµLLMæ¨¡å‹](#ä¸»æµllmæ¨¡å‹)
4. [LLMåœ¨Agentä¸­çš„åº”ç”¨](#llmåœ¨agentä¸­çš„åº”ç”¨)
5. [å¦‚ä½•é›†æˆLLM](#å¦‚ä½•é›†æˆllm)
6. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ä»€ä¹ˆæ˜¯LLM

### å®šä¹‰

**LLMï¼ˆLarge Language Modelï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼‰**æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„AIæ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚å®ƒä»¬é€šè¿‡åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œå­¦ä¼šäº†è¯­è¨€çš„æ¨¡å¼ã€è¯­æ³•ã€è¯­ä¹‰å’ŒçŸ¥è¯†ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

1. **å¤§è§„æ¨¡**ï¼šå‚æ•°é‡é€šå¸¸è¾¾åˆ°æ•°åäº¿ç”šè‡³æ•°åƒäº¿
2. **é€šç”¨æ€§**ï¼šèƒ½å¤Ÿå¤„ç†å„ç§è¯­è¨€ä»»åŠ¡
3. **ä¸Šä¸‹æ–‡ç†è§£**ï¼šèƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡å’Œè¯­å¢ƒ
4. **ç”Ÿæˆèƒ½åŠ›**ï¼šèƒ½å¤Ÿç”Ÿæˆè¿è´¯ã€æœ‰æ„ä¹‰çš„æ–‡æœ¬

### LLM vs ä¼ ç»ŸNLPæ¨¡å‹

| ç‰¹æ€§ | ä¼ ç»ŸNLPæ¨¡å‹ | LLM |
|------|------------|-----|
| **è®­ç»ƒæ•°æ®** | ç‰¹å®šé¢†åŸŸæ•°æ® | å¤§è§„æ¨¡é€šç”¨æ–‡æœ¬ |
| **å‚æ•°é‡** | ç™¾ä¸‡åˆ°åƒä¸‡çº§ | åäº¿åˆ°åƒäº¿çº§ |
| **èƒ½åŠ›** | å•ä¸€ä»»åŠ¡ | å¤šä»»åŠ¡é€šç”¨ |
| **å¾®è°ƒ** | éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ® | å°‘é‡æ ·æœ¬å³å¯ï¼ˆFew-shotï¼‰ |
| **æ³›åŒ–èƒ½åŠ›** | è¾ƒå¼± | å¾ˆå¼º |

---

## LLMçš„å·¥ä½œåŸç†

### 1. é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰

LLMé¦–å…ˆåœ¨å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒï¼š

```
è®­ç»ƒæ•°æ®ï¼ˆäº’è”ç½‘æ–‡æœ¬ã€ä¹¦ç±ã€æ–‡ç« ç­‰ï¼‰
    â†“
TokenåŒ–ï¼ˆå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ï¼‰
    â†“
Transformeræ¶æ„å¤„ç†
    â†“
å­¦ä¹ è¯­è¨€æ¨¡å¼å’ŒçŸ¥è¯†
    â†“
é¢„è®­ç»ƒæ¨¡å‹
```

### 2. Transformeræ¶æ„

LLMåŸºäºTransformeræ¶æ„ï¼Œæ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š

- **Self-Attentionæœºåˆ¶**ï¼šç†è§£è¯ä¸è¯ä¹‹é—´çš„å…³ç³»
- **Feed-Forward Networks**ï¼šå¤„ç†ä¿¡æ¯
- **Layer Normalization**ï¼šç¨³å®šè®­ç»ƒ
- **Positional Encoding**ï¼šç†è§£è¯åº

### 3. ç”Ÿæˆè¿‡ç¨‹

LLMç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ï¼š

```
è¾“å…¥æç¤ºï¼ˆPromptï¼‰
    â†“
TokenåŒ–
    â†“
æ¨¡å‹å¤„ç†ï¼ˆå¤šå±‚Transformerï¼‰
    â†“
è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
    â†“
é‡‡æ ·ä¸‹ä¸€ä¸ªToken
    â†“
é‡å¤ç›´åˆ°ç”Ÿæˆå®Œæ•´æ–‡æœ¬
```

### 4. ä¸Šä¸‹æ–‡çª—å£

LLMèƒ½å¤Ÿå¤„ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™ï¼š

- **GPT-3.5**: 4K tokens
- **GPT-4**: 8K/32K tokens
- **Claude 3**: 200K tokens
- **GPT-4 Turbo**: 128K tokens

---

## ä¸»æµLLMæ¨¡å‹

### OpenAIç³»åˆ—

#### GPT-3.5
- **ç‰¹ç‚¹**ï¼šæ€§ä»·æ¯”é«˜ï¼Œé€Ÿåº¦å¿«
- **é€‚ç”¨åœºæ™¯**ï¼šæ—¥å¸¸å¯¹è¯ã€ç®€å•ä»»åŠ¡
- **API**: `gpt-3.5-turbo`

#### GPT-4
- **ç‰¹ç‚¹**ï¼šèƒ½åŠ›å¼ºï¼Œæ¨ç†èƒ½åŠ›å¥½
- **é€‚ç”¨åœºæ™¯**ï¼šå¤æ‚ä»»åŠ¡ã€ä»£ç ç”Ÿæˆã€åˆ†æ
- **API**: `gpt-4`, `gpt-4-turbo-preview`

### Anthropicç³»åˆ—

#### Claude 3
- **ç‰¹ç‚¹**ï¼šå®‰å…¨æ€§å¥½ï¼Œä¸Šä¸‹æ–‡é•¿
- **é€‚ç”¨åœºæ™¯**ï¼šé•¿æ–‡æ¡£å¤„ç†ã€å®‰å…¨æ•æ„Ÿåº”ç”¨
- **API**: `claude-3-opus`, `claude-3-sonnet`, `claude-3-haiku`

### å¼€æºæ¨¡å‹

#### Llama 2/3 (Meta)
- **ç‰¹ç‚¹**ï¼šå¼€æºï¼Œå¯æœ¬åœ°éƒ¨ç½²
- **é€‚ç”¨åœºæ™¯**ï¼šç§æœ‰éƒ¨ç½²ã€æˆæœ¬æ§åˆ¶

#### Mistral
- **ç‰¹ç‚¹**ï¼šæ€§èƒ½å¥½ï¼Œå¼€æº
- **é€‚ç”¨åœºæ™¯**ï¼šå•†ä¸šåº”ç”¨

---

## LLMåœ¨Agentä¸­çš„åº”ç”¨

### 1. ä»»åŠ¡ç†è§£

Agentä½¿ç”¨LLMç†è§£ç”¨æˆ·çš„ç›®æ ‡ï¼š

```typescript
const prompt = `
ç”¨æˆ·ç›®æ ‡ï¼š${goal}
è¯·å°†è¿™ä¸ªç›®æ ‡åˆ†è§£ä¸ºå…·ä½“çš„ä»»åŠ¡åˆ—è¡¨ã€‚
`;

const tasks = await llm.generate(prompt);
```

### 2. ä»»åŠ¡è§„åˆ’

Agentä½¿ç”¨LLMè§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼š

```typescript
const prompt = `
ç›®æ ‡ï¼š${goal}
å·²å®Œæˆä»»åŠ¡ï¼š${completedTasks}
å½“å‰ä»»åŠ¡ï¼š${currentTask}

åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè§„åˆ’ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
`;

const plan = await llm.generate(prompt);
```

### 3. å·¥å…·é€‰æ‹©

Agentä½¿ç”¨LLMé€‰æ‹©åˆé€‚çš„å·¥å…·ï¼š

```typescript
const prompt = `
ä»»åŠ¡ï¼š${task}
å¯ç”¨å·¥å…·ï¼š${availableTools}

é€‰æ‹©æœ€é€‚åˆçš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚
`;

const selectedTool = await llm.selectTool(prompt, availableTools);
```

### 4. ç»“æœåˆ†æ

Agentä½¿ç”¨LLMåˆ†ææ‰§è¡Œç»“æœï¼š

```typescript
const prompt = `
ä»»åŠ¡ï¼š${task}
æ‰§è¡Œç»“æœï¼š${result}

åˆ†æè¿™ä¸ªç»“æœï¼Œåˆ¤æ–­ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆã€‚
`;

const analysis = await llm.analyze(prompt);
```

### 5. æ–°ä»»åŠ¡ç”Ÿæˆ

Agentä½¿ç”¨LLMç”Ÿæˆæ–°ä»»åŠ¡ï¼š

```typescript
const prompt = `
ç›®æ ‡ï¼š${goal}
å·²å®Œæˆï¼š${completedTasks}
æœ€åç»“æœï¼š${lastResult}

ç”Ÿæˆä¸‹ä¸€æ­¥éœ€è¦æ‰§è¡Œçš„ä»»åŠ¡ã€‚
`;

const newTasks = await llm.generateTasks(prompt);
```

---

## å¦‚ä½•é›†æˆLLM

### æ–¹æ³•1ï¼šä½¿ç”¨OpenAI API

#### å®‰è£…ä¾èµ–

```bash
npm install openai
```

#### åŸºæœ¬ä½¿ç”¨

```typescript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

async function callLLM(prompt: string): Promise<string> {
  const response = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo',
    messages: [
      { role: 'user', content: prompt }
    ],
    temperature: 0.7,
    max_tokens: 1000
  });
  
  return response.choices[0].message.content || '';
}
```

### æ–¹æ³•2ï¼šä½¿ç”¨Anthropic API

#### å®‰è£…ä¾èµ–

```bash
npm install @anthropic-ai/sdk
```

#### åŸºæœ¬ä½¿ç”¨

```typescript
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
});

async function callLLM(prompt: string): Promise<string> {
  const message = await anthropic.messages.create({
    model: 'claude-3-sonnet-20240229',
    max_tokens: 1000,
    messages: [
      { role: 'user', content: prompt }
    ]
  });
  
  return message.content[0].type === 'text' 
    ? message.content[0].text 
    : '';
}
```

### æ–¹æ³•3ï¼šä½¿ç”¨LangChain

#### å®‰è£…ä¾èµ–

```bash
npm install langchain @langchain/openai
```

#### åŸºæœ¬ä½¿ç”¨

```typescript
import { ChatOpenAI } from '@langchain/openai';
import { HumanMessage } from '@langchain/core/messages';

const llm = new ChatOpenAI({
  modelName: 'gpt-3.5-turbo',
  temperature: 0.7
});

async function callLLM(prompt: string): Promise<string> {
  const response = await llm.invoke([
    new HumanMessage(prompt)
  ]);
  
  return response.content as string;
}
```

---

## åœ¨è‡ªä¸»Agentä¸­é›†æˆLLM

### 1. åˆ›å»ºLLMæœåŠ¡ç±»

```typescript
interface LLMService {
  generate(prompt: string): Promise<string>;
  generateTasks(goal: string, context: string): Promise<string[]>;
  analyze(result: string): Promise<string>;
}

class OpenAILLMService implements LLMService {
  private client: OpenAI;
  
  constructor(apiKey: string) {
    this.client = new OpenAI({ apiKey });
  }
  
  async generate(prompt: string): Promise<string> {
    const response = await this.client.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7
    });
    
    return response.choices[0].message.content || '';
  }
  
  async generateTasks(goal: string, context: string): Promise<string[]> {
    const prompt = `
ç›®æ ‡ï¼š${goal}
ä¸Šä¸‹æ–‡ï¼š${context}

è¯·ç”Ÿæˆ3-5ä¸ªå…·ä½“çš„ä»»åŠ¡æ¥å®Œæˆè¿™ä¸ªç›®æ ‡ã€‚æ¯ä¸ªä»»åŠ¡ä¸€è¡Œã€‚
    `;
    
    const response = await this.generate(prompt);
    return response.split('\n').filter(line => line.trim().length > 0);
  }
  
  async analyze(result: string): Promise<string> {
    const prompt = `
åˆ†æä»¥ä¸‹æ‰§è¡Œç»“æœï¼Œåˆ¤æ–­ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆï¼š
${result}
    `;
    
    return await this.generate(prompt);
  }
}
```

### 2. åœ¨Agentä¸­ä½¿ç”¨LLM

```typescript
class AutonomousAgent {
  private llm: LLMService;
  
  constructor(goal: string, llm: LLMService) {
    this.goal = goal;
    this.llm = llm;
  }
  
  async createInitialTasks(): Promise<void> {
    const prompt = `
ç›®æ ‡ï¼š${this.state.goal}

è¯·å°†è¿™ä¸ªç›®æ ‡åˆ†è§£ä¸º3-5ä¸ªå…·ä½“çš„ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡ä¸€è¡Œï¼Œæ ¼å¼ï¼šä»»åŠ¡æè¿°|ä¼˜å…ˆçº§(0-1)
    `;
    
    const response = await this.llm.generate(prompt);
    const taskLines = response.split('\n').filter(line => line.trim());
    
    const tasks = taskLines.map((line, index) => {
      const [description, priorityStr] = line.split('|');
      return {
        id: `task_${index + 1}`,
        description: description.trim(),
        priority: parseFloat(priorityStr?.trim() || '0.5'),
        dependencies: [],
        status: 'pending' as const
      };
    });
    
    this.state.tasks.push(...tasks);
  }
  
  async createNewTasks(): Promise<void> {
    const lastTask = this.state.completedTasks[this.state.completedTasks.length - 1];
    const lastResult = this.state.knowledge.get(`task_${lastTask.id}`);
    
    const context = `
å·²å®Œæˆä»»åŠ¡ï¼š${this.state.completedTasks.map(t => t.description).join(', ')}
æœ€åä»»åŠ¡ç»“æœï¼š${lastResult}
    `;
    
    const newTaskDescriptions = await this.llm.generateTasks(this.state.goal, context);
    
    const newTasks = newTaskDescriptions.map((desc, index) => ({
      id: `task_${Date.now()}_${index}`,
      description: desc,
      priority: 0.7,
      dependencies: [lastTask.id],
      status: 'pending' as const
    }));
    
    this.state.tasks.push(...newTasks);
  }
}
```

---

## æœ€ä½³å®è·µ

### 1. Promptè®¾è®¡

#### âœ… å¥½çš„Prompt

```
ç›®æ ‡ï¼šæ„å»ºä¸€ä¸ªWebåº”ç”¨
å·²å®Œæˆï¼šéœ€æ±‚åˆ†æã€æŠ€æœ¯é€‰å‹
å½“å‰ä»»åŠ¡ï¼šè®¾è®¡æ•°æ®åº“æ¶æ„

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸‹ä¸€æ­¥éœ€è¦æ‰§è¡Œçš„ä»»åŠ¡ã€‚
è¦æ±‚ï¼š
1. ä»»åŠ¡è¦å…·ä½“å¯æ‰§è¡Œ
2. è€ƒè™‘ä¾èµ–å…³ç³»
3. ä¼˜å…ˆçº§è¦åˆç†
```

#### âŒ ä¸å¥½çš„Prompt

```
ç”Ÿæˆä»»åŠ¡
```

### 2. æ¸©åº¦ï¼ˆTemperatureï¼‰è®¾ç½®

- **ä½æ¸©åº¦ï¼ˆ0.1-0.3ï¼‰**ï¼šç¡®å®šæ€§é«˜ï¼Œé€‚åˆéœ€è¦å‡†ç¡®æ€§çš„ä»»åŠ¡
- **ä¸­æ¸©åº¦ï¼ˆ0.5-0.7ï¼‰**ï¼šå¹³è¡¡ï¼Œé€‚åˆå¤§å¤šæ•°ä»»åŠ¡
- **é«˜æ¸©åº¦ï¼ˆ0.8-1.0ï¼‰**ï¼šåˆ›é€ æ€§é«˜ï¼Œé€‚åˆéœ€è¦å¤šæ ·æ€§çš„ä»»åŠ¡

### 3. Tokenç®¡ç†

```typescript
class TokenManager {
  private maxTokens = 4000;
  private usedTokens = 0;
  
  canUse(tokens: number): boolean {
    return this.usedTokens + tokens <= this.maxTokens;
  }
  
  use(tokens: number): void {
    this.usedTokens += tokens;
  }
  
  reset(): void {
    this.usedTokens = 0;
  }
}
```

### 4. é”™è¯¯å¤„ç†

```typescript
async function callLLMWithRetry(
  prompt: string,
  maxRetries = 3
): Promise<string> {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await llm.generate(prompt);
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await sleep(1000 * (i + 1)); // æŒ‡æ•°é€€é¿
    }
  }
  throw new Error('Max retries exceeded');
}
```

### 5. æˆæœ¬æ§åˆ¶

```typescript
class CostTracker {
  private totalCost = 0;
  private costPer1KTokens = 0.002; // GPT-3.5ä»·æ ¼ç¤ºä¾‹
  
  track(tokens: number): void {
    this.totalCost += (tokens / 1000) * this.costPer1KTokens;
  }
  
  getTotalCost(): number {
    return this.totalCost;
  }
  
  reset(): void {
    this.totalCost = 0;
  }
}
```

---

## å¸¸è§é—®é¢˜

### Q1: LLMè°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: å®ç°é‡è¯•æœºåˆ¶å’Œé™çº§ç­–ç•¥ï¼š

```typescript
async function callLLMWithFallback(prompt: string): Promise<string> {
  try {
    return await openaiLLM.generate(prompt);
  } catch (error) {
    console.warn('OpenAI failed, trying Claude...');
    try {
      return await claudeLLM.generate(prompt);
    } catch (error2) {
      // æœ€åçš„é™çº§æ–¹æ¡ˆ
      return 'LLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨';
    }
  }
}
```

### Q2: å¦‚ä½•æ§åˆ¶LLMçš„è¾“å‡ºæ ¼å¼ï¼Ÿ

**A**: åœ¨Promptä¸­æ˜ç¡®æŒ‡å®šæ ¼å¼ï¼š

```typescript
const prompt = `
è¯·ç”Ÿæˆä»»åŠ¡åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºJSONï¼š
{
  "tasks": [
    {"description": "ä»»åŠ¡æè¿°", "priority": 0.8}
  ]
}
`;
```

### Q3: LLMå“åº”å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: ä½¿ç”¨æµå¼å“åº”æˆ–å¼‚æ­¥å¤„ç†ï¼š

```typescript
// æµå¼å“åº”
async function* streamLLM(prompt: string) {
  const stream = await openai.chat.completions.create({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: prompt }],
    stream: true
  });
  
  for await (const chunk of stream) {
    yield chunk.choices[0]?.delta?.content || '';
  }
}
```

### Q4: å¦‚ä½•å‡å°‘Tokenæ¶ˆè€—ï¼Ÿ

**A**: 
1. å‹ç¼©Prompt
2. ä½¿ç”¨ç¼“å­˜
3. æ‰¹é‡å¤„ç†
4. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

```typescript
class PromptCache {
  private cache = new Map<string, string>();
  
  async get(prompt: string): Promise<string> {
    const key = this.hash(prompt);
    if (this.cache.has(key)) {
      return this.cache.get(key)!;
    }
    
    const result = await llm.generate(prompt);
    this.cache.set(key, result);
    return result;
  }
}
```

---

## æ€»ç»“

LLMæ˜¯Agentçš„æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒèµ‹äºˆAgentï¼š

1. **ç†è§£èƒ½åŠ›**ï¼šç†è§£ç”¨æˆ·æ„å›¾å’Œç›®æ ‡
2. **è§„åˆ’èƒ½åŠ›**ï¼šåˆ¶å®šæ‰§è¡Œè®¡åˆ’
3. **å†³ç­–èƒ½åŠ›**ï¼šé€‰æ‹©åˆé€‚çš„è¡ŒåŠ¨
4. **ç”Ÿæˆèƒ½åŠ›**ï¼šç”Ÿæˆæ–°ä»»åŠ¡å’Œå†…å®¹

é€šè¿‡åˆç†é›†æˆLLMï¼ŒAgentèƒ½å¤Ÿå®ç°çœŸæ­£çš„è‡ªä¸»æ€§å’Œæ™ºèƒ½æ€§ã€‚

---

## å‚è€ƒèµ„æ–™

- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs)
- [Anthropic APIæ–‡æ¡£](https://docs.anthropic.com/)
- [LangChainæ–‡æ¡£](https://js.langchain.com/)
- [Prompt EngineeringæŒ‡å—](https://www.promptingguide.ai/)

---

**ä¸‹ä¸€æ­¥å­¦ä¹ **ï¼š
- [ ] å­¦ä¹ Prompt EngineeringæŠ€å·§
- [ ] å­¦ä¹ å¦‚ä½•ä¼˜åŒ–LLMè°ƒç”¨
- [ ] å­¦ä¹ å¦‚ä½•åœ¨Agentä¸­æœ‰æ•ˆä½¿ç”¨LLM
